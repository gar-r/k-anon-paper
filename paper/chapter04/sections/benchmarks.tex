In this section we will present the results of the benchmarks executed on the anonymization algorithm, and compare the results to the expected algorithm characteristics introduced earlier in Chapter~\ref{ch:chapter_algorithm}. As mentioned in Section~\ref{subsec:benchmarks} benchmarks in Go are special tests, which can be executed with the testing framework. Go will help us run the analyzed function until the measure is \emph{stable}, and is able to precisely calculate the average execution time.

\subsection{Test Rig}

The configuration that was used for executing the benchmarks is summarized on Figure~\ref{fig:test_rig}.
\input{chapter04/figures/test-rig.tex}

\subsection{RangeGeneralizer benchmarks}\label{subsec:range_generalizer_benchmarks}

Figure~\ref{fig:range_bm} shows a comparison benchmark of both range based generalizers --- \texttt{IntRange} and \texttt{FloatRange}. The horizontal axis plots the size of the range, the vertical axis shows the \emph{average time to generalize the median value in the range to the highest level} in nanoseconds as measured by the Go bench framework. As expected, both show a \textbf{logarithmic} progression curve (note the logarithmic scale on the horizontal axis). Generalizing float ranges is more expensive however, this is due to the increased number of splits and additional cost of comparison with delta.

\vspace{\baselineskip}
\input{chapter04/figures/range-bm.tex}

\subsection{PrefixGeneralizer benchmarks}

The \texttt{PrefixGeneralizer} has three different parameters. For each benchmark, we will test one parameter while the other two are fixed. The tests always generalize the input to the \emph{maximum possible level}.

\subsubsection{Word Count}

In this test we set the maximum allowed words to a high enough fixed value, and benchmark the performance based on the \emph{actual number of words} (amount of text) on the input of the generalizer. Figure~\ref{fig:prefix_word_count_bm} shows a relatively flat \textbf{polynomial curve}. The performance starts to degrade after 5000 words, which is an indicator of the amount of (English) text it is supposed to be able to handle per cell in the input data table.

\vspace{\baselineskip}
\input{chapter04/figures/prefix-word-count-bm.tex}

\subsubsection{Word Length}

As discussed during the introduction of the prefix generalizer in Section~\ref{subsec:prefix_generalizer} the performance of the prefix generalization algorithm should be \emph{unaffected} by the length of words. The benchmark on Figure~\ref{fig:prefix_word_length_bm} proves this adequately. The time needed to generalize with fixed max words, word count and increasing word length assumes a \textbf{flat line}.

\vspace{\baselineskip}
\input{chapter04/figures/prefix-word-length-bm.tex}

\subsubsection{Max Words}

Finally we inspect the effect of the \texttt{MaxWords} parameter. We can recall, that this parameter increases the \emph{total levels} of the generalizer, even when the actual number of words in the generalized text is smaller. Thus the \textbf{linear curve} visible on Figure~\ref{fig:prefix_max_words_bm} seems realistic.

\vspace{\baselineskip}
\input{chapter04/figures/prefix-max-words-bm.tex}

\subsection{HierarchyGeneralizer benchmarks}

\subsubsection{Hierarchy depth}
One way hierarchies can be benchmarked is based on their depth. Given a fixed node count, the number of levels correlates to the number of children each node in the hierarchy has. Since it is easier to build a hierarchy by specifying the number of children per node, we will use this approach for our benchmark. Figure~\ref{fig:hierarchy_children_bm} shows an interesting result (note the logarithmic vertical axis). The generalization time greatly improves with a more flat hierarchy up to around 4 or 5 children. Increasing the child-count further has no significant beneficial effect. The explanation for this behavior is, that while the generalization hierarchy is not actually a search-tree, increasing the child-count thus reducing the total number of levels for the same amount of nodes will actually improve the number of steps needed to locate the partition a given item belongs to on a certain level (up to a certain threshold).
\vspace{\baselineskip}
\input{chapter04/figures/hierarchy-children-bm.tex}

\subsubsection{Number of nodes}
By increasing the number of nodes in the hierarchy, the generalization time will increase in a \textbf{linear} fashion as shown on Figure~\ref{fig:hierarchy_nodes_bm}.
\vspace{\baselineskip}
\input{chapter04/figures/hierarchy-nodes-bm.tex}

\subsection{Comparison of generalizers}

Plotting all the built-in generalizers on the same chart gives a good opportunity to compare their relative performance when the row count is being increased given a fixed set of parameters. Note, that each generalizer has several different parameters which affect the performance greatly, so the following comparisons are only approximate guidelines.

According to Figure~\ref{fig:compare_all} the slowest generalizer is the \texttt{FloatRange} generalizer. This might be a surprise in light of Section~\ref{subsec:range_generalizer_benchmarks}, which showed that the numeric range generalizers scaled really well, when considering only the parameter which controls the width of the range. Unfortunately, when having a large amount of numeric columns with a fixed range width, they have a poor performance when the number of rows in the table is increased. 

In order to get a better overview, Figure~\ref{fig:compare_fast} shows the same setup with \texttt{FloatRange} omitted.

\vspace{\baselineskip}
\input{chapter04/figures/compare-all.tex}

\vspace{\baselineskip}
\input{chapter04/figures/compare-fast.tex}

\subsection{Anonymization benchmarks}

The following benchmarks measure the anonymization of a full sample data table. The table contains only integer numbers generalized with the same \texttt{IntRange} generalizer. The benchmark variables are the \emph{column count}, the \emph{row count} and the \emph{k} anonymity parameter.

\subsubsection{Column count}

We can expect the generalization time to increase for increasing the number of columns in the table. The reason is, that we need to compare more data when calculating the \emph{scaled generalization cost} for the cost-graph. This extra amount of work however is only \textbf{linear}, as shown on Figure~\ref{fig:anon_columns}.

\vspace{\baselineskip}
\input{chapter04/figures/anon-columns.tex}

\subsubsection{Row count}

This is the most important benchmark of all of the above, as it (indirectly) measures the core implementation of the graph based approximation algorithm. As proven in Section~\ref{subsec:polynomial-time-approximation-algorithm} increasing the number of rows in the table should result in a \(\mathcal{O}(kn^2)\) curve. The benchmark on Figure~\ref{fig:anon_rows} confirms, that \textbf{our implementation of the algorithm has the same characteristics}.

\vspace{\baselineskip}
\input{chapter04/figures/anon-rows.tex}

\subsubsection{k value}

Finally, we investigate if the \emph{k} anonymity parameter has any effect on the anonymization time. In this benchmark the \emph{k} value was increased from 2 to 100 while having all other input parameters fixed. The only restriction is, that the data table needs to contain at least \emph{k} number of rows in order to be able to achieve k-anonymity. The resulting benchmark can be seen on Figure~\ref{fig:anon_k} and shows, that the \emph{k} value has no measurable effect on the anonymization time for a static sized input table.

\vspace{\baselineskip}
\input{chapter04/figures/anon-k.tex}