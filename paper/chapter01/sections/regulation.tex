Even the Big Tech companies (who not surprisingly are also the biggest collectors and consumers of personal data) are
beginning to acknowledge that personal data collection needs to be regulated~\cite{wired01}.
Recent privacy scandals of Google (Google+)~\cite{precursor01}, Facebook, Cambridge Analytica~\cite{wiki01}, Amazon~\cite{wiki02} and others~\cite{technadu01} seem to support the importance of the need for a stricter regulation as well.

More recently governments are slowly catching up on the regulation front.
One prime example is the \textit{``General Data Protection Regulation'' (GDPR)} of the European Union.
The \textit{GDPR} got some critique~\cite{clearcritique,thomsonreuters}, but it has a solid foundation.
It regulates how the collection of personal data is disclosed to the data subjects, and demands that data stored on people in the EU undergo either an \textbf{\textit{anonymization}} or a \textbf{\textit{pseudonymization}} process~\cite{wiki03, wiki04}.
The main difference between the two processes is, that pseudonymized data can be restored to its original state by re-adding identifiers, while anonymized data can not be restored.
In this document we will mostly be discussing anonymization, however it is not hard to imagine how one would construct a pseudonymization algorithm given a working anonymization algorithm.
